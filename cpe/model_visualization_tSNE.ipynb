{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You may need to run this twice due to a pip dependency conflict\n",
    "%pip install https://github.com/braceal/cpe.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'h5py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional, Tuple\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'h5py'"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from contextlib import ExitStack\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from all_cluster_visualization import PlotClustersData\n",
    "from Bio import SeqIO  # type: ignore[import]\n",
    "from main_llm import get_model, get_sequences, get_tokenizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BatchEncoding, PreTrainedTokenizerFast\n",
    "from utils import (\n",
    "    gc_content,\n",
    "    get_label_dict,\n",
    "    parse_sequence_labels,\n",
    "    preprocess_data,\n",
    "    read_fasta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODON_TO_CHAR = {\n",
    "    \"TCG\": \"A\",\n",
    "    \"GCA\": \"B\",\n",
    "    \"CTT\": \"C\",\n",
    "    \"ATT\": \"D\",\n",
    "    \"TTA\": \"E\",\n",
    "    \"GGG\": \"F\",\n",
    "    \"CGT\": \"G\",\n",
    "    \"TAA\": \"H\",\n",
    "    \"AAA\": \"I\",\n",
    "    \"CTC\": \"J\",\n",
    "    \"AGT\": \"K\",\n",
    "    \"CCA\": \"L\",\n",
    "    \"TGT\": \"M\",\n",
    "    \"GCC\": \"N\",\n",
    "    \"GTT\": \"O\",\n",
    "    \"ATA\": \"P\",\n",
    "    \"TAC\": \"Q\",\n",
    "    \"TTT\": \"R\",\n",
    "    \"TGC\": \"S\",\n",
    "    \"CAC\": \"T\",\n",
    "    \"ACG\": \"U\",\n",
    "    \"CCC\": \"V\",\n",
    "    \"ATC\": \"W\",\n",
    "    \"CAT\": \"X\",\n",
    "    \"AGA\": \"Y\",\n",
    "    \"GAG\": \"Z\",\n",
    "    \"GTG\": \"a\",\n",
    "    \"GGT\": \"b\",\n",
    "    \"GCT\": \"c\",\n",
    "    \"TTC\": \"d\",\n",
    "    \"AAC\": \"e\",\n",
    "    \"TAT\": \"f\",\n",
    "    \"GTA\": \"g\",\n",
    "    \"CCG\": \"h\",\n",
    "    \"ACA\": \"i\",\n",
    "    \"CGA\": \"j\",\n",
    "    \"TAG\": \"k\",\n",
    "    \"CTG\": \"l\",\n",
    "    \"GGA\": \"m\",\n",
    "    \"ATG\": \"n\",\n",
    "    \"TCT\": \"o\",\n",
    "    \"CGG\": \"p\",\n",
    "    \"GAT\": \"q\",\n",
    "    \"ACC\": \"r\",\n",
    "    \"GAC\": \"s\",\n",
    "    \"GTC\": \"t\",\n",
    "    \"TGG\": \"u\",\n",
    "    \"CCT\": \"v\",\n",
    "    \"GAA\": \"w\",\n",
    "    \"TCA\": \"x\",\n",
    "    \"CAA\": \"y\",\n",
    "    \"AAT\": \"z\",\n",
    "    \"ACT\": \"0\",\n",
    "    \"GCG\": \"1\",\n",
    "    \"GGC\": \"2\",\n",
    "    \"CTA\": \"3\",\n",
    "    \"AAG\": \"4\",\n",
    "    \"AGG\": \"5\",\n",
    "    \"CAG\": \"6\",\n",
    "    \"AGC\": \"7\",\n",
    "    \"CGC\": \"8\",\n",
    "    \"TTG\": \"9\",\n",
    "    \"TCC\": \"!\",\n",
    "    \"TGA\": \"@\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):  # type: ignore[type-arg]\n",
    "    \"\"\"Dataset initialized from a list of sequence strings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequences: List[str],\n",
    "        seq_length: int,\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        kmer_size: int = 3,\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        self.batch_encodings = self.tokenize_sequences(\n",
    "            sequences, tokenizer, seq_length, kmer_size, verbose\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_sequences(\n",
    "        sequences: List[str],\n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        seq_length: int,\n",
    "        kmer_size: int = 3,\n",
    "        verbose: bool = True,\n",
    "    ) -> List[BatchEncoding]:\n",
    "        tokenizer_fn = functools.partial(\n",
    "            tokenizer,\n",
    "            max_length=seq_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch_encodings = [\n",
    "            tokenizer_fn(SequenceDataset.group_and_contextualize(seq, kmer_size))\n",
    "            for seq in tqdm(sequences, desc=\"Tokenizing...\", disable=not verbose)\n",
    "        ]\n",
    "        return batch_encodings\n",
    "\n",
    "    @staticmethod\n",
    "    def group_and_contextualize(seq: str, k: int = 3):\n",
    "        return \"\".join(\n",
    "            CODON_TO_CHAR.get(seq[i : i + k], \"\") for i in range(0, len(seq), k)\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.batch_encodings)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        batch_encoding = self.batch_encodings[idx]\n",
    "        # Squeeze so that batched tensors end up with (batch_size, seq_length)\n",
    "        # instead of (batch_size, 1, seq_length)\n",
    "        sample = {\n",
    "            \"input_ids\": batch_encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": batch_encoding[\"attention_mask\"],\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the fasta filepath to a fasta path:\n",
    "fasta_path = \"\"\n",
    "seqs_raw = read_fasta(fasta_path)\n",
    "\n",
    "labels = parse_sequence_labels\n",
    "sequences, labels = preprocess_data(seqs_raw, labels)\n",
    "label_dict = get_label_dict(labels)\n",
    "\n",
    "label_categories = set(labels)\n",
    "\n",
    "# enter the checkpoint to the tokenizer:\n",
    "tokenizer_checkpoint = \"\"\n",
    "\n",
    "tokenizer = get_tokenizer(\n",
    "    sequences, tokenizer_checkpoint=tokenizer_checkpoint, vocab_size=50_257\n",
    ")\n",
    "\n",
    "# TODO: see how to get the seq_length from the args in main_llm:\n",
    "seq_length = 1024\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = get_model(\n",
    "    tokenizer=tokenizer, model_architecture=\"bert_3m\", model_checkpoint=None\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "dataset = SequenceDataset(sequences, seq_length, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        outputs = model(\n",
    "            batch[\"input_ids\"].to(device),\n",
    "            batch[\"attention_mask\"].to(device),\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        # outputs.hidden_states shape: (layers, batch_size, sequence_length, hidden_size)\n",
    "        # Use the embeddings of the last layer\n",
    "        emb = outputs.hidden_states[-1].detach().cpu().numpy()\n",
    "        # Compute average over sequence length\n",
    "        emb = np.mean(emb, axis=1)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "# Concatenate embeddings into an array of shape (num_sequences, hidden_size)\n",
    "embeddings = np.concatenate(embeddings)\n",
    "\n",
    "# embeddings should be of size (N, hidden_size)\n",
    "print(embeddings.shape, \": shape of embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_embeddings = TSNE(n_components=2).fit_transform(embeddings)\n",
    "label_array = np.array([label_dict[x] for x in labels])\n",
    "gc_content_of_seqs = np.array(gc_content(sequences))\n",
    "\n",
    "embedding_visualization = PlotClustersData(\n",
    "    tsne_embeddings, label_array, gc_content_of_seqs, label_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting ALL clusters colored with label\n",
    "\n",
    "(\n",
    "    plot_df_separate,\n",
    "    hue_separate,\n",
    "    plt_title,\n",
    ") = embedding_visualization.separate_clusters_labels()\n",
    "embedding_visualization.plot_clusters(plot_df_separate, hue_separate, plt_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting all points colored with gc content\n",
    "for x in range(len(label_categories)):\n",
    "    (\n",
    "        plot_df_separate_gc_coding,\n",
    "        hue,\n",
    "        plt_title,\n",
    "    ) = embedding_visualization.separate_clusters_gc_content(label_mask=x)\n",
    "    embedding_visualization.plot_clusters(plot_df_separate_gc_coding, hue, plt_title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting all clusters colored with gc content\n",
    "\n",
    "(\n",
    "    plot_df_separate,\n",
    "    hue_separate,\n",
    "    plt_title,\n",
    ") = embedding_visualization.plot_both_clusters_gc_content()\n",
    "embedding_visualization.plot_clusters(plot_df_separate, hue_separate, plt_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    embeddings, labels, stratify=labels, random_state=1\n",
    ")\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "print(f\"MLP model train accuracy: {clf.score(X_train, y_train)}\")\n",
    "print(f\"MLP model test accuracy: {clf.score(X_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
