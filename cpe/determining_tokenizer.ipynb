{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/couchbucks/Documents/saketh/cpe/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import queue\n",
    "import pandas as pd\n",
    "import threading\n",
    "from typing import Union, List\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path \n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio.Seq import translate\n",
    "PathLike = Union[str, Path]\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "\n",
    "#from genslm.utils import read_fasta_only_seq, read_fasta, Sequence\n",
    "\n",
    "# Assign a unique character to each codon so that we can use it as an\n",
    "# input token to a BPE tokenizer. This implements a codon-pair encoding.\n",
    "CODON_CHAR = {\n",
    "    'TCG': \"A\", 'GCA': \"B\", 'CTT': \"C\", 'ATT': \"D\", 'TTA': \"E\", 'GGG': \"F\", 'CGT': \"G\", \n",
    "    'TAA': \"H\", 'AAA': \"I\", 'CTC': \"J\", 'AGT': \"K\", 'CCA': \"L\", 'TGT': \"M\", 'GCC': \"N\", \n",
    "    'GTT': \"O\", 'ATA': \"P\", 'TAC': \"Q\", 'TTT': \"R\", 'TGC': \"S\", 'CAC': \"T\", 'ACG': \"U\", \n",
    "    'CCC': \"V\", 'ATC': \"W\", 'CAT': \"X\", 'AGA': \"Y\", 'GAG': \"Z\", 'GTG': \"a\", 'GGT': \"b\", \n",
    "    'GCT': \"c\", 'TTC': \"d\", 'AAC': \"e\", 'TAT': \"f\", 'GTA': \"g\", 'CCG': \"h\", 'ACA': \"i\", \n",
    "    'CGA': \"j\", 'TAG': \"k\", 'CTG': \"l\", 'GGA': \"m\", 'ATG': \"n\", 'TCT': \"o\", 'CGG': \"p\", \n",
    "    'GAT': \"q\", 'ACC': \"r\", 'GAC': \"s\", 'GTC': \"t\", 'TGG': \"u\", 'CCT': \"v\", 'GAA': \"w\", \n",
    "    'TCA': \"x\", 'CAA': \"y\", 'AAT': \"z\", 'ACT': \"0\", 'GCG': \"1\", 'GGC': \"2\", 'CTA': \"3\", \n",
    "    'AAG': \"4\", 'AGG': \"5\", 'CAG': \"6\", 'AGC': \"7\", 'CGC': \"8\", 'TTG': \"9\", 'TCC': \"!\", \n",
    "    'TGA': \"@\"\n",
    "}\n",
    "\n",
    "def group_and_contextualize_cpe(seq: str, k: int = 3): \n",
    "    return \" \".join(CODON_CHAR.get(seq[i : i + k], \"\") for i in range(0, len(seq), k))\n",
    "\n",
    "def group_and_contextualize_split_every_n(seq: str, n):\n",
    "    seq.replace(\" \", \"\")\n",
    "    remainder = len(seq) % n\n",
    "    if remainder != 0:\n",
    "        seq = seq[:-remainder]  # Remove the last remainder characters\n",
    "    substrings = [seq[i:i + n] for i in range(0, len(seq), n)]\n",
    "    return ' '.join(substrings)\n",
    "\n",
    "\n",
    "def build_tokenizer(\n",
    "    corpus_iterator,\n",
    "    vocab_size,\n",
    "    add_bos_eos: bool = True,\n",
    "    max_length: int = 1024,\n",
    "    save: bool = False,\n",
    "    tokenzier_save_name: str = \"cpe_tokenizer\",\n",
    "):\n",
    "    special_tokens = {\n",
    "        \"unk_token\": \"[UNK]\",\n",
    "        \"cls_token\": \"[CLS]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"mask_token\": \"[MASK]\",\n",
    "        \"bos_token\": \"[BOS]\",\n",
    "        \"eos_token\": \"[EOS]\",\n",
    "    }\n",
    "\n",
    "    bos_index = 5\n",
    "    eos_index = 6\n",
    "\n",
    "    # Define tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    \n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=list(special_tokens.values()),\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    print(\"Training tokenizer\")\n",
    "    tokenizer.train_from_iterator(corpus_iterator, trainer=trainer)\n",
    "\n",
    "    # Add post-processor\n",
    "    # trim_offsets=True will ignore spaces, false will leave them in\n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "    if add_bos_eos:\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[BOS] $A [EOS]\",\n",
    "            special_tokens=[(\"[BOS]\", bos_index), (\"[EOS]\", eos_index)],\n",
    "        )\n",
    "\n",
    "    # Add a decoder\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    # save the tokenizer\n",
    "    wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer, **special_tokens\n",
    "    )\n",
    "    if save:\n",
    "        wrapped_tokenizer.save_pretrained(tokenzier_save_name)\n",
    "        \n",
    "    print(f\"Returning tokenizer with vocab_size = {vocab_size}\")\n",
    "\n",
    "    return wrapped_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta_only_seq(fasta_file: PathLike) -> List[str]:\n",
    "    \"\"\"Reads fasta file sequences without description tag.\"\"\"\n",
    "    text = Path(fasta_file).read_text()\n",
    "    pattern = re.compile(\"^>\", re.MULTILINE)\n",
    "    non_parsed_seqs = re.split(pattern, text)[1:]\n",
    "    lines = [\n",
    "        line.replace(\"\\n\", \"\") for seq in non_parsed_seqs for line in seq.split(\"\\n\", 1)\n",
    "    ]\n",
    "\n",
    "    return lines[1::2]\n",
    "\n",
    "def any_file_fasta_reader(fasta_file: PathLike) -> List[str]:\n",
    "    if os.path.isdir(fasta_file):\n",
    "        sequences_raw = []\n",
    "        for p in Path(fasta_file).glob(\"*.fasta\"):\n",
    "            sequences_raw.extend(read_fasta_only_seq(p))\n",
    "    elif os.path.isfile(fasta_file):\n",
    "        sequences_raw = read_fasta_only_seq(fasta_file)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Kindly enter a filepath to a directory containing many .fasta files \"\n",
    "            \"or a filepath to a single .fasta file\"\n",
    "        )\n",
    "    \n",
    "    return sequences_raw\n",
    "        \n",
    "\n",
    "def group_and_contextualize_split_every_n(seq: str, n):\n",
    "    seq.replace(\" \", \"\")\n",
    "    remainder = len(seq) % n\n",
    "    if remainder != 0:\n",
    "        seq = seq[:-remainder]  # Remove the last remainder characters\n",
    "    substrings = [seq[i:i + n] for i in range(0, len(seq), n)]\n",
    "    return ' '.join(substrings)\n",
    "\n",
    "def group_and_contextualize_cpe(seq: str, k: int = 3): \n",
    "    return \" \".join(CODON_CHAR.get(seq[i : i + k], \"\") for i in range(0, len(seq), k))\n",
    "\n",
    "def make_str_div(seq, n):\n",
    "    remainder = len(seq) % n\n",
    "    #print(remainder)\n",
    "    if remainder != 0:\n",
    "        seq = seq[:-remainder]\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vocab_lens(sequence_fasta_file: str, max_vocab_len: int, increment: int, bpe_tokenizer_type: str):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        sequences [str]: _description_\n",
    "        max_vocab_len (int): _description_\n",
    "        increment (int): _description_\n",
    "        bpe_tokenizer_type (str): choices: [\"ape_tokenizer\", \"cpe_tokenizer\", \"npe_tokenizer\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    dna_sequences = any_file_fasta_reader(sequence_fasta_file)\n",
    "    \n",
    "    \n",
    "     # data preprocessing dependent on tokenizer_type\n",
    "    if bpe_tokenizer_type == \"ape_tokenizer\":\n",
    "        sequences = [group_and_contextualize_split_every_n(translate(make_str_div(seq.upper()), 1)) for seq in dna_sequences]\n",
    "    \n",
    "    elif bpe_tokenizer_type == \"cpe_tokenizer\":\n",
    "        sequences = [group_and_contextualize_cpe(make_str_div(seq.upper(), 3)) for seq in dna_sequences]\n",
    "        \n",
    "    elif bpe_tokenizer_type == \"npe_tokenizer\":\n",
    "        sequences = [group_and_contextualize_split_every_n(make_str_div(seq.upper(), 1)) for seq in dna_sequences]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Please enter a valid bpe_tokenizer_type :)\")\n",
    "\n",
    "\n",
    "    num_tokens_in_vocab = {}\n",
    "    per_vocab_in_vocab = {}\n",
    "    \n",
    "    for vocab_size in range(increment, max_vocab_len, increment):\n",
    "        tokenizer = build_tokenizer(sequences, vocab_size)\n",
    "        \n",
    "        unique_lens = []\n",
    "        print(f\"Finding lens for {vocab_size}\")\n",
    "        for seq in tqdm(sequences):\n",
    "            \n",
    "            tokenized_seq = tokenizer(seq, max_length=1024, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "            #print(torch.unique(tokenized_seq[\"input_ids\"]))\n",
    "            unique_lens.append(len(torch.unique(tokenized_seq['input_ids'])))\n",
    "\n",
    "        #print(unique_lens)\n",
    "        \n",
    "        avg_num_unique_tokens = (sum(unique_lens)/len(unique_lens))\n",
    "        \n",
    "        num_tokens_in_vocab[vocab_size] = avg_num_unique_tokens\n",
    "        \n",
    "        \n",
    "    for key, value in num_tokens_in_vocab.items():\n",
    "        per_vocab_in_vocab[key] = value/key\n",
    "    \n",
    "    \n",
    "    plt.scatter(num_tokens_in_vocab.keys(), num_tokens_in_vocab.values())\n",
    "    plt.title(bpe_tokenizer_type + \" absolute avg num unique tokens\")\n",
    "    plt.xlabel(\"Vocab Size\")\n",
    "    plt.ylabel(\"Num Unique Tokens Used\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(per_vocab_in_vocab.keys(), per_vocab_in_vocab.values())\n",
    "    plt.title(bpe_tokenizer_type + \" relative avg num unique tokens\")\n",
    "    plt.xlabel(\"Vocab Size\")\n",
    "    plt.ylabel(\"Percentage of Vocab Size Used\")\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "group_and_contextualize_split_every_n() missing 1 required positional argument: 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_vocab_lens\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_fasta_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/couchbucks/Downloads/all_fasta_files/mdh_natural_sequences.ffn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_vocab_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbpe_tokenizer_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnpe_tokenizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mplot_vocab_lens\u001b[0;34m(sequence_fasta_file, max_vocab_len, increment, bpe_tokenizer_type)\u001b[0m\n\u001b[1;32m     19\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m [group_and_contextualize_cpe(make_str_div(seq\u001b[38;5;241m.\u001b[39mupper(), \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m dna_sequences]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m bpe_tokenizer_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpe_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m [group_and_contextualize_split_every_n(make_str_div(seq\u001b[38;5;241m.\u001b[39mupper(), \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m dna_sequences]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease enter a valid bpe_tokenizer_type :)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m [group_and_contextualize_cpe(make_str_div(seq\u001b[38;5;241m.\u001b[39mupper(), \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m dna_sequences]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m bpe_tokenizer_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpe_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m [\u001b[43mgroup_and_contextualize_split_every_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_str_div\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m dna_sequences]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease enter a valid bpe_tokenizer_type :)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: group_and_contextualize_split_every_n() missing 1 required positional argument: 'n'"
     ]
    }
   ],
   "source": [
    "plot_vocab_lens(sequence_fasta_file='/home/couchbucks/Downloads/all_fasta_files/mdh_natural_sequences.ffn', max_vocab_len=100, increment=30, bpe_tokenizer_type='npe_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,1,1,1,1,1,1,1,1,1,1])\n",
    "print(torch.unique(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "print(len(torch.tensor([ 3,  5,  6,  7,  8,  9, 10, 11, 12, 14, 15, 16, 17, 19, 20, 21, 22, 24,\n",
    "        25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 41, 42, 44, 45, 46,\n",
    "        47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65,\n",
    "        67, 68, 69, 70])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    }
   ],
   "source": [
    "a_list = []\n",
    "a_list.append(9)\n",
    "print(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "868329871d340e68a723f22f0d183ed267f7fc5205a50eb76781ebde7044a33d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
