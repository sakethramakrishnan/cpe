bpe_vocab_size: 200
convert_to_aa: true
data_path: data/sample_c_data
eval_steps: 1
evaluation_strategy: steps
fp16: true
gradient_accumulation_steps: 2
hidden_size: 128
learning_rate: 5.0e-05
logging_steps: 500
logging_strategy: steps
max_length: 512
mlm: true
mlm_probability: 0.15
model_architecture: ''
model_path: ''
num_attention_heads: 2
num_char_per_token: 1
num_layers: 2
num_train_epochs: 20
num_workers: 4
output_dir: testing
per_device_eval_batch_size: 10
per_device_test_batch_size: 10
per_device_train_batch_size: 10
run_name: testing_biological_tokenizers
save_steps: 500
save_total_limit: 1
task_type: classification
tokenizer_path: tokenizer_json_files/protein_alphabet_wordlevel.json
tokenizer_type: protein_alphabet_wordlevel
wandb_project: ''
warmup_steps: 1000
weight_decay: 0.01
