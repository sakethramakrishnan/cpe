eval_steps: 1500
evaluation_strategy: steps
fp16: false
gradient_accumulation_steps: 2
learning_rate: 5.0e-05
load_best_model_at_end: true
logging_steps: 500
logging_strategy: steps
num_train_epochs: 250
output_dir: /lus/eagle/projects/RL-fold/sakisubi/cpe/cpe/checkpoints/baselines/codon_wordlevel/neox/neox_33m_codon_baseline_mdh
per_device_eval_batch_size: 64
per_device_train_batch_size: 64
save_steps: 1500
save_total_limit: 5
tokenizer_path: /lus/eagle/projects/RL-fold/sakisubi/cpe/cpe/tokenizer_json_files/codon_wordlevel_71vocab.json
train_path: /lus/eagle/projects/RL-fold/sakisubi/cpe/data/datasets/mdh/train.fasta
validation_path: /lus/eagle/projects/RL-fold/sakisubi/cpe/data/datasets/mdh/valid.fasta
wandb_project: neox_33m_codon_baseline_mdh
warmup_steps: 1000
weight_decay: 0.01

# if tokenizer = ape_tokenizer; use:
# convert_to_aa: True
# num_char_per_token: 1

# if tokenizer = cpe_tokenizer; use:=
# convert_to_aa: False
# num_char_per_token: 3

# if tokenizer = npe_tokenizer; use:
# convert_to_aa: False
# num_char_per_token: 1

# if tokenizer = codon_wordlevel; use:
# convert_to_aa: False
# num_char_per_token: 3

# if tokenizer = dna_wordlevel; use:
# convert_to_aa: False
# num_char_per_token: 1

# if tokenizer = protein_alphabet_wordlevel; use:
# convert_to_aa: True
# num_char_per_token: 1

convert_to_aa: False
num_char_per_token: 3
tokenizer_type: codon_wordlevel
model_architecture: 'neox'
model_path: "/lus/eagle/projects/RL-fold/sakisubi/cpe/cpe/bert/neox_33m.json"
