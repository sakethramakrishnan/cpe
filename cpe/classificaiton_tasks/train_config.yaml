bpe_vocab_size: 200
convert_to_aa: false
data_path: data/sample_data
eval_steps: 1
evaluation_strategy: steps
fp16: true
gradient_accumulation_steps: 2
hidden_size: 128
learning_rate: 5.0e-05
logging_steps: 500
logging_strategy: steps
max_length: 512
mlm: true
mlm_probability: 0.15
model_architecture: ''
model_path: ''
num_attention_heads: 2
num_char_per_token: 3
num_layers: 2
num_train_epochs: 20
num_workers: 4
output_dir: ''
per_device_eval_batch_size: 2
per_device_test_batch_size: 2
per_device_train_batch_size: 2
save_steps: 500
save_total_limit: 1
task_type: regression
tokenizer_path: tokenizer_json_files/codon_wordlevel_71vocab.json
tokenizer_type: codon_wordlevel
wandb_project: bpe_tokenizers
warmup_steps: 1000
weight_decay: 0.01
