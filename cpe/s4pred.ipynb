{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy.typing as npt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import BatchEncoding, PreTrainedTokenizerFast, BertForMaskedLM\n",
    "\n",
    "from dataset import FastaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy.typing as npt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import BatchEncoding, PreTrainedTokenizerFast, BertForMaskedLM\n",
    "\n",
    "from dataset import FastaDataset\n",
    "\n",
    "CODON_TO_CHAR = {\n",
    "    \"TCG\": \"A\",\n",
    "    \"GCA\": \"B\",\n",
    "    \"CTT\": \"C\",\n",
    "    \"ATT\": \"D\",\n",
    "    \"TTA\": \"E\",\n",
    "    \"GGG\": \"F\",\n",
    "    \"CGT\": \"G\",\n",
    "    \"TAA\": \"H\",\n",
    "    \"AAA\": \"I\",\n",
    "    \"CTC\": \"J\",\n",
    "    \"AGT\": \"K\",\n",
    "    \"CCA\": \"L\",\n",
    "    \"TGT\": \"M\",\n",
    "    \"GCC\": \"N\",\n",
    "    \"GTT\": \"O\",\n",
    "    \"ATA\": \"P\",\n",
    "    \"TAC\": \"Q\",\n",
    "    \"TTT\": \"R\",\n",
    "    \"TGC\": \"S\",\n",
    "    \"CAC\": \"T\",\n",
    "    \"ACG\": \"U\",\n",
    "    \"CCC\": \"V\",\n",
    "    \"ATC\": \"W\",\n",
    "    \"CAT\": \"X\",\n",
    "    \"AGA\": \"Y\",\n",
    "    \"GAG\": \"Z\",\n",
    "    \"GTG\": \"a\",\n",
    "    \"GGT\": \"b\",\n",
    "    \"GCT\": \"c\",\n",
    "    \"TTC\": \"d\",\n",
    "    \"AAC\": \"e\",\n",
    "    \"TAT\": \"f\",\n",
    "    \"GTA\": \"g\",\n",
    "    \"CCG\": \"h\",\n",
    "    \"ACA\": \"i\",\n",
    "    \"CGA\": \"j\",\n",
    "    \"TAG\": \"k\",\n",
    "    \"CTG\": \"l\",\n",
    "    \"GGA\": \"m\",\n",
    "    \"ATG\": \"n\",\n",
    "    \"TCT\": \"o\",\n",
    "    \"CGG\": \"p\",\n",
    "    \"GAT\": \"q\",\n",
    "    \"ACC\": \"r\",\n",
    "    \"GAC\": \"s\",\n",
    "    \"GTC\": \"t\",\n",
    "    \"TGG\": \"u\",\n",
    "    \"CCT\": \"v\",\n",
    "    \"GAA\": \"w\",\n",
    "    \"TCA\": \"x\",\n",
    "    \"CAA\": \"y\",\n",
    "    \"AAT\": \"z\",\n",
    "    \"ACT\": \"0\",\n",
    "    \"GCG\": \"1\",\n",
    "    \"GGC\": \"2\",\n",
    "    \"CTA\": \"3\",\n",
    "    \"AAG\": \"4\",\n",
    "    \"AGG\": \"5\",\n",
    "    \"CAG\": \"6\",\n",
    "    \"AGC\": \"7\",\n",
    "    \"CGC\": \"8\",\n",
    "    \"TTG\": \"9\",\n",
    "    \"TCC\": \"!\",\n",
    "    \"TGA\": \"@\",\n",
    "    \"XXX\": \"*\",\n",
    "}\n",
    "\n",
    "# enter the fasta filepath to a fasta path:\n",
    "fasta_path = \"../data/datasets/sample_fasta.fasta\"\n",
    "\n",
    "# enter the checkpoint to the tokenizer:\n",
    "tokenizer_path = \"cpe_tokenizer_retrained_3000\"\n",
    "\n",
    "\n",
    "model_checkpoint = \"checkpoints/bpe/cpe_tokenizer/bert/checkpoint-34000\"\n",
    "\n",
    "# ImportError: cannot import name 'GenSLMColatorForLanguageModeling' from 'dataset' (/home/couchbucks/Documents/saketh/cpe/cpe/dataset.py)\n",
    "from transformers import BatchEncoding, DataCollatorForLanguageModeling\n",
    "class GenSLMColatorForLanguageModeling(DataCollatorForLanguageModeling):\n",
    "    \"\"\"Augment the underlying DataCollatorForLanguageModeling to handle\n",
    "    multiple batch encoding inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, train_mode: bool = False, **kwargs) -> None:\n",
    "        self.train_mode = train_mode\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def tokenize(self, sequences: List[str]) -> BatchEncoding:\n",
    "        return self.tokenizer(\n",
    "            sequences,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_special_tokens_mask=self.train_mode and self.mlm,\n",
    "            max_length=1024)\n",
    "\n",
    "    def torch_call(self, examples: List[str]) -> Dict[str, Any]:\n",
    "        # First, tokenize the batch\n",
    "        batch = self.tokenize(examples)\n",
    "        \n",
    "        # We only need to mask tokens if we are training\n",
    "        if not self.train_mode:\n",
    "            return batch\n",
    "\n",
    "        if self.mlm:\n",
    "            # If special token mask has been preprocessed, pop it from the dict.\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n",
    "                batch[\"input_ids\"],\n",
    "                special_tokens_mask=batch.pop(\"special_tokens_mask\", None),\n",
    "            )\n",
    "        else:\n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  5,  58, 543,  ...,   3,   3,   3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:01<00:02,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   5,  179, 1268,  ...,    3,    3,    3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:01<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   5,  103, 1974,  ...,    3,    3,    3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_embeddings_and_logits(model, dataloader):\n",
    "    embeddings = []\n",
    "    lsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            print(batch)\n",
    "            batch = batch.to(model.device)\n",
    "            outputs = model(**batch, output_hidden_states=True)\n",
    "            last_hidden_states = outputs.hidden_states[-1].cpu().numpy()\n",
    "        #    seq_lengths = batch.attention_mask.sum(axis=1)\n",
    "            embeddings.append(last_hidden_states)\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "def llm_inference(\n",
    "    tokenizer_path: Path,\n",
    "    model_path: Path,\n",
    "    fasta_path: Path,\n",
    "    return_codon: bool,\n",
    "    return_aminoacid: bool,\n",
    "    batch_size: int,\n",
    "    fasta_contains_aminoacid: bool = False,\n",
    ") -> Tuple[npt.ArrayLike, npt.ArrayLike, npt.ArrayLike]:\n",
    "    \n",
    "    \n",
    "    if os.path.isfile(Path(tokenizer_path)):\n",
    "        # These are for the .json files\n",
    "        tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "            pretrained_model_name_or_path=tokenizer_path\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # These are for the bpe tokenizers\n",
    "        tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
    "        \n",
    "    special_tokens = {\n",
    "            \"unk_token\": \"[UNK]\",\n",
    "            \"cls_token\": \"[CLS]\",\n",
    "            \"sep_token\": \"[SEP]\",\n",
    "            \"pad_token\": \"[PAD]\",\n",
    "            \"mask_token\": \"[MASK]\",\n",
    "            \"bos_token\": \"[BOS]\",\n",
    "            \"eos_token\": \"[EOS]\",\n",
    "        }\n",
    "\n",
    "        # for some reason, we need to add the special tokens even though they are in the json file\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "    model = BertForMaskedLM.from_pretrained(model_checkpoint)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device).eval()\n",
    "\n",
    "    if fasta_contains_aminoacid:\n",
    "        dataset = FastaAminoAcidDataset(file_path=fasta_path)\n",
    "    else:\n",
    "        dataset = FastaDataset(\n",
    "            file_path=fasta_path,\n",
    "            num_char_per_token = 3,\n",
    "            convert_to_aa = False,\n",
    "            tokenizer_type = \"cpe_tokenizer\"\n",
    "        )\n",
    "\n",
    "    data_collator = GenSLMColatorForLanguageModeling(\n",
    "        train_mode=False,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=data_collator,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    embeddings = generate_embeddings_and_logits(model, dataloader)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings= llm_inference(\n",
    "    tokenizer_path,\n",
    "    model_checkpoint,\n",
    "    fasta_path,\n",
    "    return_codon = False,\n",
    "    return_aminoacid = False,\n",
    "    batch_size = 1,\n",
    "    fasta_contains_aminoacid = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  5,  97, 523,  ...,   3,   3,   3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
    "tokenizer(\n",
    "            \"aaas;gq8934LAGQ#$Iawenfo;\",\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "           # return_special_tokens_mask=self.train_mode and self.mlm,\n",
    "            max_length=1024,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[-0.23334244,  1.3598535 , -1.421997  , ..., -0.08454225,\n",
       "          -1.5931135 , -0.34243402],\n",
       "         [-2.2037168 ,  0.06538261, -1.5209543 , ..., -1.1352464 ,\n",
       "          -1.692257  , -0.98212147],\n",
       "         [-0.8462671 ,  0.87533736, -1.2767893 , ..., -0.3343385 ,\n",
       "          -1.6808283 , -0.3408133 ],\n",
       "         ...,\n",
       "         [-0.12490162,  0.22917472, -1.2925127 , ..., -0.38309816,\n",
       "          -1.9702008 , -0.5219698 ],\n",
       "         [ 0.52098024, -0.7124421 ,  0.72963756, ...,  1.1690844 ,\n",
       "          -0.5265065 ,  0.02509315],\n",
       "         [-0.6468866 ,  0.22047757, -1.2802352 , ...,  0.2552388 ,\n",
       "          -2.201959  ,  0.05643922]]], dtype=float32),\n",
       " array([[[ 0.19624719,  0.9082316 , -1.5482137 , ..., -0.10049539,\n",
       "          -1.6603034 , -0.4468241 ],\n",
       "         [-1.7120521 ,  0.7479217 , -1.1822919 , ..., -0.08151136,\n",
       "          -0.8333578 ,  0.11761697],\n",
       "         [-0.28487328,  0.6458622 , -1.2428639 , ..., -0.09778947,\n",
       "          -1.8639753 , -0.41054076],\n",
       "         ...,\n",
       "         [-0.3222244 ,  0.43431464, -1.5327675 , ..., -0.55373776,\n",
       "          -1.780559  , -0.47414565],\n",
       "         [ 1.521998  , -0.7918071 ,  0.9956441 , ...,  1.0656708 ,\n",
       "          -1.0493783 , -0.40188572],\n",
       "         [ 0.18666995,  0.3850168 , -0.62077606, ...,  0.33503717,\n",
       "          -1.9130524 , -0.5688962 ]]], dtype=float32),\n",
       " array([[[-0.3529063 ,  1.3457989 , -1.4135776 , ..., -0.24967843,\n",
       "          -1.8792738 , -0.5033217 ],\n",
       "         [-2.2015274 ,  1.0916257 , -1.3960675 , ..., -0.21051769,\n",
       "          -0.82216513,  0.04743199],\n",
       "         [-0.8802858 ,  0.85380036, -1.1878313 , ..., -0.4159572 ,\n",
       "          -1.9254249 , -0.61151767],\n",
       "         ...,\n",
       "         [-1.9380966 ,  1.5320512 ,  0.2997722 , ...,  0.106725  ,\n",
       "          -0.75337625,  0.9985626 ],\n",
       "         [-1.471029  , -0.02963789, -0.57981944, ...,  0.73196477,\n",
       "          -2.2317488 ,  0.659308  ],\n",
       "         [ 1.0597863 ,  0.00476085, -0.12330219, ...,  0.5677506 ,\n",
       "           0.4093441 , -0.62482256]]], dtype=float32)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 1024, 240)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(embeddings).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1d2b8a7f7e8b4294e58905fa61be7044a6603711006b6ad534d224b83be168b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
