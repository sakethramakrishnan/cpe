{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c515e44c-9631-4609-b0d1-8e5e0988377e",
   "metadata": {},
   "source": [
    "# HuggingFace Codon-Pair-Ecoding Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3960257-454c-4612-a5d7-73e3ac0f672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "from genslm.utils import read_fasta, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9a9461-ddf1-453b-9813-c03384ef3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a BPE training iterator\n",
    "\n",
    "sequence_file = Path(\"/lambda_stor/homes/khippe/genslm_foundation/genome_data/mdh_sc23/fasta/mdh_natural_sequences.ffn\")\n",
    "sequences = read_fasta(sequence_file)\n",
    "\n",
    "def get_mdh_raw_sequences(): \n",
    "    for sequence in sequences: \n",
    "        yield sequence.sequence.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930d8236-da51-4055-860a-e03f390a2e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 7 sequences for non-translatable length\n"
     ]
    }
   ],
   "source": [
    "# Define cBPE training iterator\n",
    "\n",
    "# Sequence pre-processing helpers\n",
    "# Assign a unique character to each codon so that we can use it as an\n",
    "# input token to a BPE tokenizer. This implements a codon-pair encoding.\n",
    "CODON_CHAR = {\n",
    "    'TCG': \"A\", 'GCA': \"B\", 'CTT': \"C\", 'ATT': \"D\", 'TTA': \"E\", 'GGG': \"F\", 'CGT': \"G\", \n",
    "    'TAA': \"H\", 'AAA': \"I\", 'CTC': \"J\", 'AGT': \"K\", 'CCA': \"L\", 'TGT': \"M\", 'GCC': \"N\", \n",
    "    'GTT': \"O\", 'ATA': \"P\", 'TAC': \"Q\", 'TTT': \"R\", 'TGC': \"S\", 'CAC': \"T\", 'ACG': \"U\", \n",
    "    'CCC': \"V\", 'ATC': \"W\", 'CAT': \"X\", 'AGA': \"Y\", 'GAG': \"Z\", 'GTG': \"a\", 'GGT': \"b\", \n",
    "    'GCT': \"c\", 'TTC': \"d\", 'AAC': \"e\", 'TAT': \"f\", 'GTA': \"g\", 'CCG': \"h\", 'ACA': \"i\", \n",
    "    'CGA': \"j\", 'TAG': \"k\", 'CTG': \"l\", 'GGA': \"m\", 'ATG': \"n\", 'TCT': \"o\", 'CGG': \"p\", \n",
    "    'GAT': \"q\", 'ACC': \"r\", 'GAC': \"s\", 'GTC': \"t\", 'TGG': \"u\", 'CCT': \"v\", 'GAA': \"w\", \n",
    "    'TCA': \"x\", 'CAA': \"y\", 'AAT': \"z\", 'ACT': \"0\", 'GCG': \"1\", 'GGC': \"2\", 'CTA': \"3\", \n",
    "    'AAG': \"4\", 'AGG': \"5\", 'CAG': \"6\", 'AGC': \"7\", 'CGC': \"8\", 'TTG': \"9\", 'TCC': \"!\", \n",
    "    'TGA': \"@\"\n",
    "}\n",
    "\n",
    "def group_and_contextualize(seq: str, k: int = 3): \n",
    "    grouped_codons = \" \".join(seq[i : i + k] for i in range(0, len(seq), k)).upper()\n",
    "    # Removes all modulo 3 chars\n",
    "    return  \"\".join(CODON_CHAR.get(codon, \"\") for codon in grouped_codons.split())\n",
    "\n",
    "\n",
    "def decode_grouped_context(seq: str, sep: str = \" \"): \n",
    "    return sep.join(CHAR_CODON[elem] for elem in seq) \n",
    "\n",
    "\n",
    "def group_by_kmer(seq: str, kmer: int) -> str:\n",
    "    return \" \".join(seq[i : i + kmer] for i in range(0, len(seq), kmer)).upper()\n",
    "\n",
    "\n",
    "# Define iterator \n",
    "grouped_sequences = [] # Group by 3-mer (codon)\n",
    "skipped = 0\n",
    "for seq in sequences: \n",
    "    if len(seq.sequence)%3 != 0: \n",
    "        skipped += 1\n",
    "        continue \n",
    "    grouped_sequences.append(group_by_kmer(seq.sequence, 3)) \n",
    "    \n",
    "codon_pairs = [\"\".join(CODON_CHAR[codon] for codon in seq.split()) for seq in grouped_sequences] # Convert codons to unique chars\n",
    "print(f\"Skipped {skipped} sequences for non-translatable length\")\n",
    "\n",
    "def get_mdh_codon_tokens(): \n",
    "    for seq in codon_pairs: \n",
    "        yield seq \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78443348-ab18-4305-80ec-8d3b70aafa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer(corpus_function, vocab_size = 50_257, add_bos_eos : bool=True):\n",
    "    special_tokens= {\n",
    "        \"unk_token\": \"[UNK]\", \n",
    "        \"cls_token\": \"[CLS]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"mask_token\": \"[MASK]\",\n",
    "        \"bos_token\": \"[BOS]\",\n",
    "        \"eos_token\": \"[EOS]\"\n",
    "    }\n",
    "    bos_index = 5 \n",
    "    eos_index = 6\n",
    "    \n",
    "    # Define tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=list(special_tokens.values()))\n",
    "\n",
    "    \n",
    "        \n",
    "    print(\"Training tokenizer\")\n",
    "    tokenizer.train_from_iterator(corpus_function(), trainer=trainer)\n",
    "\n",
    "    # Add post-processor \n",
    "    # trim_offsets=True will ignore spaces, false will leave them in \n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "    if add_bos_eos: \n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[BOS] $A [EOS]\",\n",
    "            special_tokens=[(\"[BOS]\", bos_index), (\"[EOS]\", eos_index)],\n",
    "        )\n",
    "\n",
    "    # Add a decoder\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    # save the tokenizer \n",
    "    wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        *special_tokens\n",
    "    )\n",
    "\n",
    "    return wrapped_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64b867-5c4d-4405-b59c-cb17b0c478f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bpe_output_path = Path(\"mdh-bpe\")\n",
    "if not bpe_output_path.exists(): \n",
    "    mdh_bpe = build_tokenizer(get_mdh_raw_sequences, vocab_size = 50_257, add_bos_eos=True)\n",
    "    mdh_bpe.save_pretrained(bpe_output_path)\n",
    "else: \n",
    "    print(f\"Loading from {bpe_output_path}\")\n",
    "    mdh_bpe = PreTrainedTokenizerFast.from_pretrained(bpe_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2d18a-5b8d-45cd-8970-f2546f38c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "codon_bpe_output_path = Path(\"mdh-codon-bpe\")\n",
    "\n",
    "if not codon_bpe_output_path.exists(): \n",
    "    mdh_codon_bpe = build_tokenizer(get_mdh_codon_tokens, vocab_size = 50_257, add_bos_eos=True)\n",
    "    mdh_codon_bpe.save_pretrained(codon_bpe_output_path)\n",
    "else: \n",
    "    print(f\"Loading from {codon_bpe_output_path}\")\n",
    "    mdh_codon_bpe = PreTrainedTokenizerFast.from_pretrained(codon_bpe_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb69a56-d30d-4ec1-a7bc-dd5336c9e422",
   "metadata": {},
   "source": [
    "# Vocab Size Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001db223-18eb-452a-b36c-f7e31e6c645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mdh-codon-bpe-vs30000/tokenizer_config.json',\n",
       " 'mdh-codon-bpe-vs30000/special_tokens_map.json',\n",
       " 'mdh-codon-bpe-vs30000/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30k \n",
    "cBPE_30k_output_path = Path(\"mdh-codon-bpe-vs30000\")\n",
    "vocab_size=30_000\n",
    "cBPE_30k = build_tokenizer(get_mdh_codon_tokens, vocab_size = vocab_size, add_bos_eos=True)\n",
    "cBPE_30k.save_pretrained(cBPE_30k_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f4171b-6151-4829-9977-73fd9005847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mdh-codon-bpe-vs20000/tokenizer_config.json',\n",
       " 'mdh-codon-bpe-vs20000/special_tokens_map.json',\n",
       " 'mdh-codon-bpe-vs20000/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20k \n",
    "cBPE_20k_output_path = Path(\"mdh-codon-bpe-vs20000\")\n",
    "vocab_size=20_000\n",
    "cBPE_30k = build_tokenizer(get_mdh_codon_tokens, vocab_size = vocab_size, add_bos_eos=True)\n",
    "cBPE_30k.save_pretrained(cBPE_20k_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a4518bc-38c9-49a2-9920-048eae00d5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mdh-codon-bpe-vs10000/tokenizer_config.json',\n",
       " 'mdh-codon-bpe-vs10000/special_tokens_map.json',\n",
       " 'mdh-codon-bpe-vs10000/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10k \n",
    "cBPE_10k_output_path = Path(\"mdh-codon-bpe-vs10000\")\n",
    "vocab_size=10_000\n",
    "cBPE_30k = build_tokenizer(get_mdh_codon_tokens, vocab_size = vocab_size, add_bos_eos=True)\n",
    "cBPE_30k.save_pretrained(cBPE_10k_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dceb0a-344e-4ca4-bc14-67e8256aad2e",
   "metadata": {},
   "source": [
    "# Full Patric BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412484d2-687a-487d-b0fa-95a358362199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "patric_sequence_file = Path(\"/lambda_stor/homes/khippe/genslm_foundation/genome_data/curriculum_datasets/homology-90-50/codon_homology/pgfam_30k_all.ffn\")\n",
    "patric_sequences = read_fasta(patric_sequence_file)\n",
    "print(f\"Read {len(patric_sequences)} sequences\")\n",
    "\n",
    "# Define iterator \n",
    "patric_codon_pairs = [group_and_contextualize(seq.sequence) for seq in patric_sequences]\n",
    "del patric_sequences # free memory\n",
    "\n",
    "def get_patric_codon_tokens(): \n",
    "    for seq in patric_codon_pairs: \n",
    "        yield seq \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd09585-f668-49cc-a2f3-3936adb27154",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "patric_cBPE_50k_output_path = Path(\"patric-codon-bpe-vs50257\")\n",
    "patric_cBPE_50k = build_tokenizer(get_patric_codon_tokens, vocab_size = vocab_size, add_bos_eos=True)\n",
    "patric_cBPE_50k.save_pretrained(patric_cBPE_50k_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9d0aa-9a29-4f4b-918f-0edc84de0b33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e51f82-6f7c-4122-b39a-82e72af05792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56cd9b7-0d00-4afb-a7ef-a9bcff8dbf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import accelerate\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BertConfig, BertForMaskedLM \n",
    "\n",
    "\n",
    "from genslm.utils import read_fasta, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d4919-04cd-45b5-9a2e-44f8aef25091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tokenizer\n",
    "special_tokens= {\n",
    "        \"unk_token\": \"[UNK]\", \n",
    "        \"cls_token\": \"[CLS]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"mask_token\": \"[MASK]\",\n",
    "        \"bos_token\": \"[BOS]\",\n",
    "        \"eos_token\": \"[EOS]\"\n",
    "    }\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"mdh-codon-bpe\")\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16288186-32ad-4608-a564-3336dd86f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence pre-processing helpers\n",
    "\n",
    "CODON_MAP = {\n",
    "    'TCG': 0, 'GCA': 1, 'CTT': 2, 'ATT': 3, 'TTA': 4, 'GGG': 5, 'CGT': 6, \n",
    "    'TAA': 7, 'AAA': 8, 'CTC': 9, 'AGT': 10, 'CCA': 11, 'TGT': 12, 'GCC': 13, \n",
    "    'GTT': 14, 'ATA': 15, 'TAC': 16, 'TTT': 17, 'TGC': 18, 'CAC': 19, 'ACG': 20, \n",
    "    'CCC': 21, 'ATC': 22, 'CAT': 23, 'AGA': 24, 'GAG': 25, 'GTG': 26, 'GGT': 27, \n",
    "    'GCT': 28, 'TTC': 29, 'AAC': 30, 'TAT': 31, 'GTA': 32, 'CCG': 33, 'ACA': 34, \n",
    "    'CGA': 35, 'TAG': 36, 'CTG': 37, 'GGA': 38, 'ATG': 39, 'TCT': 40, 'CGG': 41, \n",
    "    'GAT': 42, 'ACC': 43, 'GAC': 44, 'GTC': 45, 'TGG': 46, 'CCT': 47, 'GAA': 48, \n",
    "    'TCA': 49, 'CAA': 50, 'AAT': 51, 'ACT': 52, 'GCG': 53, 'GGC': 54, 'CTA': 55, \n",
    "    'AAG': 56, 'AGG': 57, 'CAG': 58, 'AGC': 59, 'CGC': 60, 'TTG': 61, 'TCC': 62, \n",
    "    'TGA': 63\n",
    "}\n",
    "\n",
    "# Assign a unique character to each codon so that we can use it as an\n",
    "# input token to a BPE tokenizer. This implements a codon-pair encoding.\n",
    "CODON_CHAR = {\n",
    "    'TCG': \"A\", 'GCA': \"B\", 'CTT': \"C\", 'ATT': \"D\", 'TTA': \"E\", 'GGG': \"F\", 'CGT': \"G\", \n",
    "    'TAA': \"H\", 'AAA': \"I\", 'CTC': \"J\", 'AGT': \"K\", 'CCA': \"L\", 'TGT': \"M\", 'GCC': \"N\", \n",
    "    'GTT': \"O\", 'ATA': \"P\", 'TAC': \"Q\", 'TTT': \"R\", 'TGC': \"S\", 'CAC': \"T\", 'ACG': \"U\", \n",
    "    'CCC': \"V\", 'ATC': \"W\", 'CAT': \"X\", 'AGA': \"Y\", 'GAG': \"Z\", 'GTG': \"a\", 'GGT': \"b\", \n",
    "    'GCT': \"c\", 'TTC': \"d\", 'AAC': \"e\", 'TAT': \"f\", 'GTA': \"g\", 'CCG': \"h\", 'ACA': \"i\", \n",
    "    'CGA': \"j\", 'TAG': \"k\", 'CTG': \"l\", 'GGA': \"m\", 'ATG': \"n\", 'TCT': \"o\", 'CGG': \"p\", \n",
    "    'GAT': \"q\", 'ACC': \"r\", 'GAC': \"s\", 'GTC': \"t\", 'TGG': \"u\", 'CCT': \"v\", 'GAA': \"w\", \n",
    "    'TCA': \"x\", 'CAA': \"y\", 'AAT': \"z\", 'ACT': \"0\", 'GCG': \"1\", 'GGC': \"2\", 'CTA': \"3\", \n",
    "    'AAG': \"4\", 'AGG': \"5\", 'CAG': \"6\", 'AGC': \"7\", 'CGC': \"8\", 'TTG': \"9\", 'TCC': \"!\", \n",
    "    'TGA': \"@\"\n",
    "}\n",
    "\n",
    "def group_and_contextualize(seq: str, k: int = 3): \n",
    "    grouped_codons = \" \".join(seq[i : i + k] for i in range(0, len(seq), k)).upper()\n",
    "    # Removes all modulo 3 chars\n",
    "    return  \"\".join(CODON_CHAR.get(codon, \"\") for codon in grouped_codons.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b13a08-c90d-4e7c-b45f-5b642af09fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset \n",
    "sequence_file = Path(\"/lambda_stor/homes/khippe/genslm_foundation/genome_data/mdh_sc23/fasta/mdh_natural_sequences.ffn\")\n",
    "sequences = read_fasta(sequence_file)\n",
    "\n",
    "\n",
    "\n",
    "dataset_seqs = [group_and_contextualize(seq.sequence) for seq in sequences]\n",
    "tokenized_seqs = tokenizer(dataset_seqs, max_length=1024, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"input_ids\": tokenized_seqs.input_ids.tolist(), \n",
    "    \"attention_mask\": tokenized_seqs.attention_mask.tolist()\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "dataset = dataset.train_test_split(test_size=0.05)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596c784-139b-4920-abf3-743101e9bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling \n",
    "\n",
    "# 124,690,513 params\n",
    "config = BertConfig(\n",
    "    hidden_size=512, \n",
    "    num_hidden_layers=8, \n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=2048,\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    "    max_position_embeddings=1024,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = BertForMaskedLM(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"BERT size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d818f4-efdf-4554-a7ba-ab75e25db8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mdh-cBPE-BERT-50m\",\n",
    "    per_device_train_batch_size=28,\n",
    "    per_device_eval_batch_size=28,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=25,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed44540-2f05-4128-9958-f46a2631bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd57b7-e6f7-47f1-a7ea-50adba1026f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling \n",
    "\n",
    "# 124,690,513 params\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    "    max_position_embeddings=1024,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = BertForMaskedLM(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"BERT size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True)\n",
    "\n",
    "# Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mdh-cBPE-BERT-125m\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=25,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd0b2a-86e2-434d-bdeb-b892c8248e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
