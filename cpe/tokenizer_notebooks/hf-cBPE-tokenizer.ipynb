{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c515e44c-9631-4609-b0d1-8e5e0988377e",
   "metadata": {},
   "source": [
    "# HuggingFace Codon-Pair-Ecoding Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3960257-454c-4612-a5d7-73e3ac0f672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "from genslm.utils import read_fasta, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9a9461-ddf1-453b-9813-c03384ef3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a BPE training iterator\n",
    "\n",
    "sequence_file = Path(\n",
    "    \"/lambda_stor/homes/khippe/genslm_foundation/genome_data/mdh_sc23/fasta/mdh_natural_sequences.ffn\"\n",
    ")\n",
    "sequences = read_fasta(sequence_file)\n",
    "\n",
    "\n",
    "def get_mdh_raw_sequences():\n",
    "    for sequence in sequences:\n",
    "        yield sequence.sequence.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930d8236-da51-4055-860a-e03f390a2e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 7 sequences for non-translatable length\n"
     ]
    }
   ],
   "source": [
    "# Define cBPE training iterator\n",
    "\n",
    "# Sequence pre-processing helpers\n",
    "# Assign a unique character to each codon so that we can use it as an\n",
    "# input token to a BPE tokenizer. This implements a codon-pair encoding.\n",
    "CODON_CHAR = {\n",
    "    \"TCG\": \"A\",\n",
    "    \"GCA\": \"B\",\n",
    "    \"CTT\": \"C\",\n",
    "    \"ATT\": \"D\",\n",
    "    \"TTA\": \"E\",\n",
    "    \"GGG\": \"F\",\n",
    "    \"CGT\": \"G\",\n",
    "    \"TAA\": \"H\",\n",
    "    \"AAA\": \"I\",\n",
    "    \"CTC\": \"J\",\n",
    "    \"AGT\": \"K\",\n",
    "    \"CCA\": \"L\",\n",
    "    \"TGT\": \"M\",\n",
    "    \"GCC\": \"N\",\n",
    "    \"GTT\": \"O\",\n",
    "    \"ATA\": \"P\",\n",
    "    \"TAC\": \"Q\",\n",
    "    \"TTT\": \"R\",\n",
    "    \"TGC\": \"S\",\n",
    "    \"CAC\": \"T\",\n",
    "    \"ACG\": \"U\",\n",
    "    \"CCC\": \"V\",\n",
    "    \"ATC\": \"W\",\n",
    "    \"CAT\": \"X\",\n",
    "    \"AGA\": \"Y\",\n",
    "    \"GAG\": \"Z\",\n",
    "    \"GTG\": \"a\",\n",
    "    \"GGT\": \"b\",\n",
    "    \"GCT\": \"c\",\n",
    "    \"TTC\": \"d\",\n",
    "    \"AAC\": \"e\",\n",
    "    \"TAT\": \"f\",\n",
    "    \"GTA\": \"g\",\n",
    "    \"CCG\": \"h\",\n",
    "    \"ACA\": \"i\",\n",
    "    \"CGA\": \"j\",\n",
    "    \"TAG\": \"k\",\n",
    "    \"CTG\": \"l\",\n",
    "    \"GGA\": \"m\",\n",
    "    \"ATG\": \"n\",\n",
    "    \"TCT\": \"o\",\n",
    "    \"CGG\": \"p\",\n",
    "    \"GAT\": \"q\",\n",
    "    \"ACC\": \"r\",\n",
    "    \"GAC\": \"s\",\n",
    "    \"GTC\": \"t\",\n",
    "    \"TGG\": \"u\",\n",
    "    \"CCT\": \"v\",\n",
    "    \"GAA\": \"w\",\n",
    "    \"TCA\": \"x\",\n",
    "    \"CAA\": \"y\",\n",
    "    \"AAT\": \"z\",\n",
    "    \"ACT\": \"0\",\n",
    "    \"GCG\": \"1\",\n",
    "    \"GGC\": \"2\",\n",
    "    \"CTA\": \"3\",\n",
    "    \"AAG\": \"4\",\n",
    "    \"AGG\": \"5\",\n",
    "    \"CAG\": \"6\",\n",
    "    \"AGC\": \"7\",\n",
    "    \"CGC\": \"8\",\n",
    "    \"TTG\": \"9\",\n",
    "    \"TCC\": \"!\",\n",
    "    \"TGA\": \"@\",\n",
    "    \"XXX\": \"*\"\n",
    "}\n",
    "\n",
    "\n",
    "def group_and_contextualize(seq: str, k: int = 3):\n",
    "    grouped_codons = \" \".join(seq[i : i + k] for i in range(0, len(seq), k)).upper()\n",
    "    # Removes all modulo 3 chars\n",
    "    return \"\".join(CODON_CHAR.get(codon, \"\") for codon in grouped_codons.split())\n",
    "\n",
    "\n",
    "def decode_grouped_context(seq: str, sep: str = \" \"):\n",
    "    return sep.join(CHAR_CODON[elem] for elem in seq)\n",
    "\n",
    "\n",
    "def group_by_kmer(seq: str, kmer: int) -> str:\n",
    "    return \" \".join(seq[i : i + kmer] for i in range(0, len(seq), kmer)).upper()\n",
    "\n",
    "\n",
    "# Define iterator\n",
    "grouped_sequences = []  # Group by 3-mer (codon)\n",
    "skipped = 0\n",
    "for seq in sequences:\n",
    "    if len(seq.sequence) % 3 != 0:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    grouped_sequences.append(group_by_kmer(seq.sequence, 3))\n",
    "\n",
    "codon_pairs = [\n",
    "    \"\".join(CODON_CHAR[codon] for codon in seq.split()) for seq in grouped_sequences\n",
    "]  # Convert codons to unique chars\n",
    "print(f\"Skipped {skipped} sequences for non-translatable length\")\n",
    "\n",
    "\n",
    "def get_mdh_codon_tokens():\n",
    "    for seq in codon_pairs:\n",
    "        yield seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78443348-ab18-4305-80ec-8d3b70aafa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer(corpus_function, vocab_size=50_257, add_bos_eos: bool = True):\n",
    "    special_tokens = {\n",
    "        \"unk_token\": \"[UNK]\",\n",
    "        \"cls_token\": \"[CLS]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"mask_token\": \"[MASK]\",\n",
    "        \"bos_token\": \"[BOS]\",\n",
    "        \"eos_token\": \"[EOS]\",\n",
    "    }\n",
    "    bos_index = 5\n",
    "    eos_index = 6\n",
    "\n",
    "    # Define tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size, special_tokens=list(special_tokens.values())\n",
    "    )\n",
    "\n",
    "    print(\"Training tokenizer\")\n",
    "    tokenizer.train_from_iterator(corpus_function(), trainer=trainer)\n",
    "\n",
    "    # Add post-processor\n",
    "    # trim_offsets=True will ignore spaces, false will leave them in\n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "    if add_bos_eos:\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=\"[BOS] $A [EOS]\",\n",
    "            special_tokens=[(\"[BOS]\", bos_index), (\"[EOS]\", eos_index)],\n",
    "        )\n",
    "\n",
    "    # Add a decoder\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    # save the tokenizer\n",
    "    wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer, *special_tokens\n",
    "    )\n",
    "\n",
    "    return wrapped_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64b867-5c4d-4405-b59c-cb17b0c478f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_output_path = Path(\"mdh-bpe\")\n",
    "if not bpe_output_path.exists():\n",
    "    mdh_bpe = build_tokenizer(\n",
    "        get_mdh_raw_sequences, vocab_size=50_257, add_bos_eos=True\n",
    "    )\n",
    "    mdh_bpe.save_pretrained(bpe_output_path)\n",
    "else:\n",
    "    print(f\"Loading from {bpe_output_path}\")\n",
    "    mdh_bpe = PreTrainedTokenizerFast.from_pretrained(bpe_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2d18a-5b8d-45cd-8970-f2546f38c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "codon_bpe_output_path = Path(\"mdh-codon-bpe\")\n",
    "\n",
    "if not codon_bpe_output_path.exists():\n",
    "    mdh_codon_bpe = build_tokenizer(\n",
    "        get_mdh_codon_tokens, vocab_size=50_257, add_bos_eos=True\n",
    "    )\n",
    "    mdh_codon_bpe.save_pretrained(codon_bpe_output_path)\n",
    "else:\n",
    "    print(f\"Loading from {codon_bpe_output_path}\")\n",
    "    mdh_codon_bpe = PreTrainedTokenizerFast.from_pretrained(codon_bpe_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb69a56-d30d-4ec1-a7bc-dd5336c9e422",
   "metadata": {},
   "source": [
    "# Vocab Size Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001db223-18eb-452a-b36c-f7e31e6c645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mdh-codon-bpe-vs30000/tokenizer_config.json',\n",
       " 'mdh-codon-bpe-vs30000/special_tokens_map.json',\n",
       " 'mdh-codon-bpe-vs30000/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30k\n",
    "cBPE_30k_output_path = Path(\"mdh-codon-bpe-vs30000\")\n",
    "vocab_size = 30_000\n",
    "cBPE_30k = build_tokenizer(\n",
    "    get_mdh_codon_tokens, vocab_size=vocab_size, add_bos_eos=True\n",
    ")\n",
    "cBPE_30k.save_pretrained(cBPE_30k_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f4171b-6151-4829-9977-73fd9005847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mdh-codon-bpe-vs20000/tokenizer_config.json',\n",
       " 'mdh-codon-bpe-vs20000/special_tokens_map.json',\n",
       " 'mdh-codon-bpe-vs20000/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20k\n",
    "cBPE_20k_output_path = Path(\"mdh-codon-bpe-vs20000\")\n",
    "vocab_size = 20_000\n",
    "cBPE_30k = build_tokenizer(\n",
    "    get_mdh_codon_tokens, vocab_size=vocab_size, add_bos_eos=True\n",
    ")\n",
    "cBPE_30k.save_pretrained(cBPE_20k_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a4518bc-38c9-49a2-9920-048eae00d5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('mdh-codon-bpe-vs10000/tokenizer_config.json',\n",
       " 'mdh-codon-bpe-vs10000/special_tokens_map.json',\n",
       " 'mdh-codon-bpe-vs10000/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10k\n",
    "cBPE_10k_output_path = Path(\"mdh-codon-bpe-vs10000\")\n",
    "vocab_size = 10_000\n",
    "cBPE_30k = build_tokenizer(\n",
    "    get_mdh_codon_tokens, vocab_size=vocab_size, add_bos_eos=True\n",
    ")\n",
    "cBPE_30k.save_pretrained(cBPE_10k_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dceb0a-344e-4ca4-bc14-67e8256aad2e",
   "metadata": {},
   "source": [
    "# Full Patric BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412484d2-687a-487d-b0fa-95a358362199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "patric_sequence_file = Path(\n",
    "    \"/lambda_stor/homes/khippe/genslm_foundation/genome_data/curriculum_datasets/homology-90-50/codon_homology/pgfam_30k_all.ffn\"\n",
    ")\n",
    "patric_sequences = read_fasta(patric_sequence_file)\n",
    "print(f\"Read {len(patric_sequences)} sequences\")\n",
    "\n",
    "# Define iterator\n",
    "patric_codon_pairs = [group_and_contextualize(seq.sequence) for seq in patric_sequences]\n",
    "del patric_sequences  # free memory\n",
    "\n",
    "\n",
    "def get_patric_codon_tokens():\n",
    "    for seq in patric_codon_pairs:\n",
    "        yield seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd09585-f668-49cc-a2f3-3936adb27154",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "patric_cBPE_50k_output_path = Path(\"patric-codon-bpe-vs50257\")\n",
    "patric_cBPE_50k = build_tokenizer(\n",
    "    get_patric_codon_tokens, vocab_size=vocab_size, add_bos_eos=True\n",
    ")\n",
    "patric_cBPE_50k.save_pretrained(patric_cBPE_50k_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9d0aa-9a29-4f4b-918f-0edc84de0b33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e51f82-6f7c-4122-b39a-82e72af05792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56cd9b7-0d00-4afb-a7ef-a9bcff8dbf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import accelerate\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "\n",
    "from genslm.utils import read_fasta, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d4919-04cd-45b5-9a2e-44f8aef25091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tokenizer\n",
    "special_tokens = {\n",
    "    \"unk_token\": \"[UNK]\",\n",
    "    \"cls_token\": \"[CLS]\",\n",
    "    \"sep_token\": \"[SEP]\",\n",
    "    \"pad_token\": \"[PAD]\",\n",
    "    \"mask_token\": \"[MASK]\",\n",
    "    \"bos_token\": \"[BOS]\",\n",
    "    \"eos_token\": \"[EOS]\",\n",
    "}\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"mdh-codon-bpe\")\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16288186-32ad-4608-a564-3336dd86f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence pre-processing helpers\n",
    "\n",
    "CODON_MAP = {\n",
    "    \"TCG\": 0,\n",
    "    \"GCA\": 1,\n",
    "    \"CTT\": 2,\n",
    "    \"ATT\": 3,\n",
    "    \"TTA\": 4,\n",
    "    \"GGG\": 5,\n",
    "    \"CGT\": 6,\n",
    "    \"TAA\": 7,\n",
    "    \"AAA\": 8,\n",
    "    \"CTC\": 9,\n",
    "    \"AGT\": 10,\n",
    "    \"CCA\": 11,\n",
    "    \"TGT\": 12,\n",
    "    \"GCC\": 13,\n",
    "    \"GTT\": 14,\n",
    "    \"ATA\": 15,\n",
    "    \"TAC\": 16,\n",
    "    \"TTT\": 17,\n",
    "    \"TGC\": 18,\n",
    "    \"CAC\": 19,\n",
    "    \"ACG\": 20,\n",
    "    \"CCC\": 21,\n",
    "    \"ATC\": 22,\n",
    "    \"CAT\": 23,\n",
    "    \"AGA\": 24,\n",
    "    \"GAG\": 25,\n",
    "    \"GTG\": 26,\n",
    "    \"GGT\": 27,\n",
    "    \"GCT\": 28,\n",
    "    \"TTC\": 29,\n",
    "    \"AAC\": 30,\n",
    "    \"TAT\": 31,\n",
    "    \"GTA\": 32,\n",
    "    \"CCG\": 33,\n",
    "    \"ACA\": 34,\n",
    "    \"CGA\": 35,\n",
    "    \"TAG\": 36,\n",
    "    \"CTG\": 37,\n",
    "    \"GGA\": 38,\n",
    "    \"ATG\": 39,\n",
    "    \"TCT\": 40,\n",
    "    \"CGG\": 41,\n",
    "    \"GAT\": 42,\n",
    "    \"ACC\": 43,\n",
    "    \"GAC\": 44,\n",
    "    \"GTC\": 45,\n",
    "    \"TGG\": 46,\n",
    "    \"CCT\": 47,\n",
    "    \"GAA\": 48,\n",
    "    \"TCA\": 49,\n",
    "    \"CAA\": 50,\n",
    "    \"AAT\": 51,\n",
    "    \"ACT\": 52,\n",
    "    \"GCG\": 53,\n",
    "    \"GGC\": 54,\n",
    "    \"CTA\": 55,\n",
    "    \"AAG\": 56,\n",
    "    \"AGG\": 57,\n",
    "    \"CAG\": 58,\n",
    "    \"AGC\": 59,\n",
    "    \"CGC\": 60,\n",
    "    \"TTG\": 61,\n",
    "    \"TCC\": 62,\n",
    "    \"TGA\": 63,\n",
    "}\n",
    "\n",
    "# Assign a unique character to each codon so that we can use it as an\n",
    "# input token to a BPE tokenizer. This implements a codon-pair encoding.\n",
    "CODON_CHAR = {\n",
    "    \"TCG\": \"A\",\n",
    "    \"GCA\": \"B\",\n",
    "    \"CTT\": \"C\",\n",
    "    \"ATT\": \"D\",\n",
    "    \"TTA\": \"E\",\n",
    "    \"GGG\": \"F\",\n",
    "    \"CGT\": \"G\",\n",
    "    \"TAA\": \"H\",\n",
    "    \"AAA\": \"I\",\n",
    "    \"CTC\": \"J\",\n",
    "    \"AGT\": \"K\",\n",
    "    \"CCA\": \"L\",\n",
    "    \"TGT\": \"M\",\n",
    "    \"GCC\": \"N\",\n",
    "    \"GTT\": \"O\",\n",
    "    \"ATA\": \"P\",\n",
    "    \"TAC\": \"Q\",\n",
    "    \"TTT\": \"R\",\n",
    "    \"TGC\": \"S\",\n",
    "    \"CAC\": \"T\",\n",
    "    \"ACG\": \"U\",\n",
    "    \"CCC\": \"V\",\n",
    "    \"ATC\": \"W\",\n",
    "    \"CAT\": \"X\",\n",
    "    \"AGA\": \"Y\",\n",
    "    \"GAG\": \"Z\",\n",
    "    \"GTG\": \"a\",\n",
    "    \"GGT\": \"b\",\n",
    "    \"GCT\": \"c\",\n",
    "    \"TTC\": \"d\",\n",
    "    \"AAC\": \"e\",\n",
    "    \"TAT\": \"f\",\n",
    "    \"GTA\": \"g\",\n",
    "    \"CCG\": \"h\",\n",
    "    \"ACA\": \"i\",\n",
    "    \"CGA\": \"j\",\n",
    "    \"TAG\": \"k\",\n",
    "    \"CTG\": \"l\",\n",
    "    \"GGA\": \"m\",\n",
    "    \"ATG\": \"n\",\n",
    "    \"TCT\": \"o\",\n",
    "    \"CGG\": \"p\",\n",
    "    \"GAT\": \"q\",\n",
    "    \"ACC\": \"r\",\n",
    "    \"GAC\": \"s\",\n",
    "    \"GTC\": \"t\",\n",
    "    \"TGG\": \"u\",\n",
    "    \"CCT\": \"v\",\n",
    "    \"GAA\": \"w\",\n",
    "    \"TCA\": \"x\",\n",
    "    \"CAA\": \"y\",\n",
    "    \"AAT\": \"z\",\n",
    "    \"ACT\": \"0\",\n",
    "    \"GCG\": \"1\",\n",
    "    \"GGC\": \"2\",\n",
    "    \"CTA\": \"3\",\n",
    "    \"AAG\": \"4\",\n",
    "    \"AGG\": \"5\",\n",
    "    \"CAG\": \"6\",\n",
    "    \"AGC\": \"7\",\n",
    "    \"CGC\": \"8\",\n",
    "    \"TTG\": \"9\",\n",
    "    \"TCC\": \"!\",\n",
    "    \"TGA\": \"@\",\n",
    "    \"XXX\": \"*\"\n",
    "}\n",
    "\n",
    "\n",
    "def group_and_contextualize(seq: str, k: int = 3):\n",
    "    grouped_codons = \" \".join(seq[i : i + k] for i in range(0, len(seq), k)).upper()\n",
    "    # Removes all modulo 3 chars\n",
    "    return \"\".join(CODON_CHAR.get(codon, \"\") for codon in grouped_codons.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b13a08-c90d-4e7c-b45f-5b642af09fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "sequence_file = Path(\n",
    "    \"/lambda_stor/homes/khippe/genslm_foundation/genome_data/mdh_sc23/fasta/mdh_natural_sequences.ffn\"\n",
    ")\n",
    "sequences = read_fasta(sequence_file)\n",
    "\n",
    "\n",
    "dataset_seqs = [group_and_contextualize(seq.sequence) for seq in sequences]\n",
    "tokenized_seqs = tokenizer(\n",
    "    dataset_seqs,\n",
    "    max_length=1024,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"input_ids\": tokenized_seqs.input_ids.tolist(),\n",
    "    \"attention_mask\": tokenized_seqs.attention_mask.tolist(),\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "dataset = dataset.train_test_split(test_size=0.05)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596c784-139b-4920-abf3-743101e9bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "\n",
    "# 124,690,513 params\n",
    "config = BertConfig(\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=2048,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_position_embeddings=1024,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = BertForMaskedLM(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"BERT size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d818f4-efdf-4554-a7ba-ab75e25db8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mdh-cBPE-BERT-50m\",\n",
    "    per_device_train_batch_size=28,\n",
    "    per_device_eval_batch_size=28,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=25,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed44540-2f05-4128-9958-f46a2631bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd57b7-e6f7-47f1-a7ea-50adba1026f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "\n",
    "# 124,690,513 params\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    max_position_embeddings=1024,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = BertForMaskedLM(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"BERT size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True)\n",
    "\n",
    "# Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mdh-cBPE-BERT-125m\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=25,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd0b2a-86e2-434d-bdeb-b892c8248e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1d2b8a7f7e8b4294e58905fa61be7044a6603711006b6ad534d224b83be168b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
