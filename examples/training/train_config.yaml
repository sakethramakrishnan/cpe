eval_steps: 1
evaluation_strategy: steps
fp16: false
gradient_accumulation_steps: 2
learning_rate: 5.0e-05
load_best_model_at_end: true
logging_steps: 500
logging_strategy: steps
num_train_epochs: 50
output_dir: checkpoints/baselines/bert_baseline_codon_3m_mdh
per_device_eval_batch_size: 64
per_device_train_batch_size: 32
save_steps: 2
save_total_limit: 5
tokenizer_path: tokenizer_json_files/codon_wordlevel_71vocab.json
train_path: ../data/datasets/mdh_natural_dataset.fasta
validation_path: ../data/datasets/mdh_natural_dataset.fasta
wandb_project: bert_baseline_codon_3m_mdh
warmup_steps: 1000
weight_decay: 0.01

# if tokenizer = ape_tokenizer; use:
# convert_to_aa: True
# num_char_per_token: 1

# if tokenizer = cpe_tokenizer; use:=
# convert_to_aa: False
# num_char_per_token: 3

# if tokenizer = npe_tokenizer; use:
# convert_to_aa: False
# num_char_per_token: 1

# if tokenizer = codon_wordlevel; use:
# convert_to_aa: False
# num_char_per_token: 3

# if tokenizer = dna_wordlevel; use:
# convert_to_aa: False
# num_char_per_token: 1

# if tokenizer = protein_alphabet_wordlevel; use:
# convert_to_aa: True
# num_char_per_token: 1

convert_to_aa: False
num_char_per_token: 3
tokenizer_type: codon_wordlevel
model_architecture: 'bert'
model_path: "bert/bert_3m.json"
